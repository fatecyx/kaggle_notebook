{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用GalTransl进行翻译\n",
    "\n",
    "- **项目地址**: [https://github.com/xd2333/GalTransl](https://github.com/xd2333/GalTransl)\n",
    "\n",
    "## 使用方法\n",
    "\n",
    "* 导入 `fatecyx/galtransl-7b` 模型。\n",
    "  * 使用其他模型注意替换MODEL_PATH = \"/kaggle/input/galtransl-7b/gguf/galtransl-7b-v2-q6_k/1/GalTransl-7B-v2-Q6_K.gguf\"\n",
    "\n",
    "* 导入 `fatecyx/LLMServer` 和 `fatecyx/GalTransl` 数据集。\n",
    "\n",
    "* 通过datasets上传galtransl工程目录，替换以下CP命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/project0308 /kaggle/working/galtransl_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/galtransl/GalTransl /kaggle/working/GalTransl\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 执行 ln -s 命令\n",
    "try:\n",
    "    subprocess.run([\"ln\", \"-s\", sys.executable, \"/kaggle/working/GalTransl/env/bin/python\"], check=True)\n",
    "    subprocess.run([\"ln\", \"-s\", sys.executable, \"/kaggle/working/GalTransl/env/bin/python3\"], check=True)\n",
    "    subprocess.run([\"ln\", \"-s\", sys.executable, \"/kaggle/working/GalTransl/env/bin/python3.10\"], check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"创建符号链接时出错: {e}\")\n",
    "!ls -l /kaggle/working/GalTransl/env/bin/python*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/llmserver/LLMServer /kaggle/working/LLMServer\n",
    "%cd /kaggle/working/LLMServer\n",
    "!pip install \"diskcache>=5.6.1\"\n",
    "#!pip install llama-cpp-python -i https://sakurallm.github.io/llama-cpp-python/whl/cu121\n",
    "!pip install llama-cpp-python==v0.2.90 -i https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
    "!pip install -q -r requirements.llamacpp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ngrok：本地HTTP服务用于中途下载（一般用不上）\n",
    "# # !pip install -q pyngrok\n",
    "# ngrokToken = \"\"\n",
    "\n",
    "# PORT = 8000\n",
    "# DIRECTORY = \"/kaggle/working\"\n",
    "# def local_http():\n",
    "#     import http.server\n",
    "#     import socketserver\n",
    "\n",
    "#     Handler = http.server.SimpleHTTPRequestHandler\n",
    "#     Handler.directory = DIRECTORY\n",
    "\n",
    "#     with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n",
    "#         print(\"HTTP server is running at port\", PORT)\n",
    "#         print(\"Server directory is\", DIRECTORY)\n",
    "#         httpd.serve_forever()\n",
    "\n",
    "# if ngrokToken:\n",
    "#     import threading\n",
    "#     threading.Thread(target=local_http, daemon=True, args=()).start()\n",
    "        \n",
    "#     from pyngrok import conf, ngrok\n",
    "#     conf.get_default().auth_token = ngrokToken\n",
    "#     conf.get_default().monitor_thread = False\n",
    "#     ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n",
    "#     if len(ssh_tunnels) == 0:\n",
    "#         ssh_tunnel = ngrok.connect(PORT)\n",
    "#         print('address：'+ssh_tunnel.public_url)\n",
    "#     else:\n",
    "#         print('address：'+ssh_tunnels[0].public_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动sakura模型\n",
    "%cd /kaggle/working/LLMServer\n",
    "from pathlib import Path\n",
    "MODEL_PATH = \"/kaggle/input/galtransl-7b/gguf/galtransl-7b-v3-q5_k_s/1/GalTransl-7B-v3-Q5_K_S.gguf\"\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "def local_model(dic_status):\n",
    "#     cmd = f\"python server.py --model_name_or_path {MODEL_PATH} --llama_cpp --use_gpu --model_version 0.9 --trust_remote_code --no-auth\"\n",
    "#     p = subprocess.Popen(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "    # 构建命令列表\n",
    "    cmds = [\n",
    "        \"python\",\n",
    "        \"server.py\",\n",
    "        \"--model_name_or_path\",\n",
    "        MODEL_PATH,\n",
    "        \"--llama_cpp\",\n",
    "        \"--use_gpu\",\n",
    "        \"--model_version\",\n",
    "        \"0.9\",\n",
    "        \"--trust_remote_code\",\n",
    "        \"--no-auth\"\n",
    "    ]\n",
    "\n",
    "    # 使用Popen执行命令\n",
    "    p = subprocess.Popen(cmds, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "\n",
    "    for line in p.stderr:\n",
    "        if not dic_status['status']:\n",
    "            s2 = line.decode()\n",
    "            dic_status['msg'] = s2\n",
    "            if \"INFO Running on http://127.0.0.1:5000\" in s2:\n",
    "                dic_status['status'] = True\n",
    "        #print(line.decode(), end='')\n",
    "dic_status = {'status': False, 'msg': ''}\n",
    "\n",
    "threading.Thread(target=local_model, daemon=True, args=(dic_status,)).start()\n",
    "\n",
    "s = \"\"\n",
    "while not dic_status['status']:\n",
    "    if s != dic_status['msg']:\n",
    "        s = dic_status['msg']\n",
    "        print(s)\n",
    "print(\"模型启动成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 调试用1\n",
    "# import subprocess\n",
    "# try:\n",
    "#     result = subprocess.run(\n",
    "#         [\"/kaggle/working/GalTransl/env/bin/python\", \n",
    "#         \"-m\", \"GalTransl\",\n",
    "#         \"-p\", \"/kaggle/working/galtransl_project\",\n",
    "#         \"-t\", \"galtransl-v3\"],\n",
    "#         cwd=\"/kaggle/working/GalTransl\",\n",
    "#         timeout=int(11.5*3600)\n",
    "#     )\n",
    "#     print(\"Process completed successfully.\")\n",
    "# except subprocess.TimeoutExpired:\n",
    "#     print(\"Process timed out and was terminated.\")\n",
    "\n",
    "# # 调试用2\n",
    "# %cd /kaggle/working/GalTransl\n",
    "# !/kaggle/working/GalTransl/env/bin/python -m GalTransl -p /kaggle/working/galtransl_project -t galtransl-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# 定义缓冲区和时间记录\n",
    "output_buffer = []\n",
    "error_buffer = []\n",
    "last_output_time = time.time()\n",
    "\n",
    "# 设置超时时间\n",
    "timeout = int(11 * 3600)\n",
    "output_internal = 1200\n",
    "\n",
    "def read_output(pipe, buffer):\n",
    "    for line in iter(pipe.readline, b''):\n",
    "        try:\n",
    "            line = line.decode('utf-8').strip()\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if line:\n",
    "            buffer.append((time.time(), line))\n",
    "            if len(buffer) > 2:\n",
    "                buffer.pop(0)\n",
    "\n",
    "# 启动子进程\n",
    "process = subprocess.Popen(\n",
    "    [\"/kaggle/working/GalTransl/env/bin/python\", \n",
    "     \"-m\", \"GalTransl\",\n",
    "     \"-p\", \"/kaggle/working/galtransl_project\",\n",
    "     \"-t\", \"galtransl-v3\"],\n",
    "    cwd=\"/kaggle/working/GalTransl\",\n",
    "    stdout=subprocess.PIPE,  # 捕获标准输出\n",
    "    stderr=subprocess.PIPE,  # 捕获错误输出\n",
    ")\n",
    "\n",
    "# 创建线程读取标准输出和标准错误输出\n",
    "output_thread = threading.Thread(target=read_output, args=(process.stdout, output_buffer))\n",
    "error_thread = threading.Thread(target=read_output, args=(process.stderr, error_buffer))\n",
    "\n",
    "output_thread.start()\n",
    "error_thread.start()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    while process.poll() is None:\n",
    "        # 每隔1小时输出最后2行和记录的时间\n",
    "        current_time = time.time()\n",
    "        if current_time - last_output_time >= output_internal:\n",
    "            \n",
    "            # 获取最新的输出和错误\n",
    "            combined_output = output_buffer + error_buffer\n",
    "            # 过滤掉小于 last_output_time 的行\n",
    "            combined_output = [item for item in combined_output if item[0] >= last_output_time]\n",
    "            # 按时间排序\n",
    "            combined_output.sort(key=lambda x: x[0])\n",
    "            \n",
    "            last_output_time = current_time\n",
    "            \n",
    "            # 输出日志\n",
    "            print(f\"Output at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}:\")\n",
    "            for timestamp, line in combined_output:\n",
    "                print(line)\n",
    "            \n",
    "            # 输出第一条和最后一条的时间\n",
    "            if combined_output:\n",
    "                first_timestamp = combined_output[0][0]\n",
    "                last_timestamp = combined_output[-1][0]\n",
    "                print(f\"First log time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(first_timestamp))}\")\n",
    "                print(f\"Last log time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(last_timestamp))}\")\n",
    "        \n",
    "        \n",
    "        # 检查是否超时\n",
    "        if current_time - start_time >= timeout:\n",
    "            raise subprocess.TimeoutExpired(process.args, timeout)\n",
    "            \n",
    "        time.sleep(1)\n",
    "    \n",
    "    # 等待线程完成\n",
    "    output_thread.join()\n",
    "    error_thread.join()\n",
    "\n",
    "    # 获取最新的输出和错误\n",
    "    combined_output = output_buffer + error_buffer\n",
    "    # 过滤掉小于 last_output_time 的行\n",
    "    combined_output = [item for item in combined_output if item[0] >= last_output_time]\n",
    "    # 按时间排序\n",
    "    combined_output.sort(key=lambda x: x[0])\n",
    "    \n",
    "    last_output_time = current_time\n",
    "    \n",
    "    # 输出日志\n",
    "    print(f\"Output at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}:\")\n",
    "    for timestamp, line in combined_output:\n",
    "        print(line)\n",
    "    \n",
    "    # 输出第一条和最后一条的时间\n",
    "    if combined_output:\n",
    "        first_timestamp = combined_output[0][0]\n",
    "        last_timestamp = combined_output[-1][0]\n",
    "        print(f\"First log time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(first_timestamp))}\")\n",
    "        print(f\"Last log time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(last_timestamp))}\")\n",
    "    \n",
    "    print(\"Process completed successfully.\")\n",
    "\n",
    "except subprocess.TimeoutExpired:\n",
    "    process.terminate()\n",
    "    print(\"Process timed out and was terminated.\")\n",
    "except Exception as e:\n",
    "    print(\"ProcessFailed\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 结果\n",
    "\n",
    "- 结束时会生成 `/kaggle/working/result.zip` 文件。\n",
    "- 成功结束时，日志会打印 `Process completed successfully.`。\n",
    "- 如果未完成，则会显示 `Process timed out and was terminated.`。此时可以将 `result.zip` 重新作为输入文件再次运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('/kaggle/working/result', 'zip', '/kaggle/working/galtransl_project')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5263619,
     "sourceId": 8760771,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
