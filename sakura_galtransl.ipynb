{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 使用GalTransl进行翻译\n","\n","- **项目地址**: [https://github.com/xd2333/GalTransl](https://github.com/xd2333/GalTransl)\n","\n","## 使用方法\n","\n","### 导入models\n","导入 `fatecyx/galtransl` 模型。\n","\n","### 导入datasets\n","导入 `fatecyx/llmserver` 数据集。\n","\n","### 导入galtransl工程目录\n","通过datasets导入galtransl工程目录，替换以下命令中的 `/kaggle/input/dec-i18n-project2/DEC_i18n_project`：\n","\n","```bash\n","!cp -r /kaggle/input/dec-i18n-project2/DEC_i18n_project /kaggle/working/galtransl_project\n","```\n","## 结果\n","\n","- 结束时会生成 `/kaggle/working/result.zip` 文件。\n","- 成功结束时，日志会打印 `Process completed successfully.`。\n","- 如果未完成，则会显示 `Process timed out and was terminated.`。此时可以将 `result.zip` 重新作为输入文件再次运行。"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!cp -r /kaggle/input/dec-i18n-project2/DEC_i18n_project /kaggle/working/galtransl_project\n","\n","!cp -r /kaggle/input/llmserver/LLMServer /kaggle/working/LLMServer\n","!cp -r /kaggle/input/galtransl/GalTransl /kaggle/working/GalTransl\n","\n","%cd /kaggle/working/GalTransl\n","!pip install virtualenv\n","!virtualenv env\n","!/kaggle/working/GalTransl/env/bin/pip install -r requirements.txt\n","!/kaggle/working/GalTransl/env/bin/pip install opencc==1.1.6\n","\n","%cd /kaggle/working/LLMServer\n","!pip install \"diskcache>=5.6.1\"\n","!pip install llama-cpp-python -i https://sakurallm.github.io/llama-cpp-python/whl/cu121\n","!pip install -q -r requirements.llamacpp.txt\n","!pip install -q pyngrok"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ngrok：本地HTTP服务用于中途下载（一般用不上）\n","ngrokToken = \"\"\n","\n","PORT = 8000\n","DIRECTORY = \"/kaggle/working\"\n","def local_http():\n","    import http.server\n","    import socketserver\n","\n","    Handler = http.server.SimpleHTTPRequestHandler\n","    Handler.directory = DIRECTORY\n","\n","    with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n","        print(\"HTTP server is running at port\", PORT)\n","        print(\"Server directory is\", DIRECTORY)\n","        httpd.serve_forever()\n","\n","if ngrokToken:\n","    threading.Thread(target=local_http, daemon=True, args=()).start()\n","        \n","    from pyngrok import conf, ngrok\n","    conf.get_default().auth_token = ngrokToken\n","    conf.get_default().monitor_thread = False\n","    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n","    if len(ssh_tunnels) == 0:\n","        ssh_tunnel = ngrok.connect(PORT)\n","        print('address：'+ssh_tunnel.public_url)\n","    else:\n","        print('address：'+ssh_tunnels[0].public_url)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 启动sakura模型\n","%cd /kaggle/working/LLMServer\n","from huggingface_hub import hf_hub_download\n","from pathlib import Path\n","MODEL_PATH = \"/kaggle/input/galtransl/gguf/galtransl-7b-v2-q6_k/1/GalTransl-7B-v2-Q6_K.gguf\"\n","\n","import subprocess\n","import threading\n","import time\n","\n","def local_model(dic_status):\n","    cmd = f\"python server.py --model_name_or_path {MODEL_PATH} --llama_cpp --use_gpu --model_version 0.9 --trust_remote_code --no-auth\"\n","    \n","    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n","    for line in p.stderr:\n","        if not dic_status['status']:\n","            s2 = line.decode()\n","            dic_status['msg'] = s2\n","            if \"INFO Running on http://127.0.0.1:5000\" in s2:\n","                dic_status['status'] = True\n","        #print(line.decode(), end='')\n","dic_status = {'status': False, 'msg': ''}\n","\n","threading.Thread(target=local_model, daemon=True, args=(dic_status,)).start()\n","\n","s = \"\"\n","while not dic_status['status']:\n","    if s != dic_status['msg']:\n","        s = dic_status['msg']\n","        print(s)\n","print(\"模型启动成功\")\n","\n","\n","try:\n","    result = subprocess.run(\n","        [\"/kaggle/working/GalTransl/env/bin/python\", \n","        \"-m\", \"GalTransl\",\n","        \"-p\", \"/kaggle/working/galtransl_project\",\n","        \"-t\" \"galtransl-v2\"],\n","        cwd=\"/kaggle/working/GalTransl\",\n","        stdout=subprocess.DEVNULL,  # 屏蔽标准输出\n","        stderr=subprocess.DEVNULL,  # 屏蔽错误输出\n","        timeout=int(11.5*3600)\n","    )\n","    print(\"Process completed successfully.\")\n","except subprocess.TimeoutExpired:\n","    print(\"Process timed out and was terminated.\")\n","    \n","# %cd /kaggle/working/GalTransl\n","# !/kaggle/working/GalTransl/env/bin/python -m GalTransl -p /kaggle/working/galtransl_project -t galtransl-v2\n","\n","import shutil\n","shutil.make_archive('/kaggle/working/result', 'zip', '/kaggle/working/galtransl_project')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5263619,"sourceId":8760771,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
