{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!cp -r /kaggle/input/rj01204891/myProject /kaggle/working/galtransl_project\n","\n","!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n","\n","!git clone https://github.com/xd2333/GalTransl.git\n","# !git clone --no-checkout https://github.com/xd2333/GalTransl.git\n","# %cd /kaggle/working/GalTransl\n","# !git checkout f43a369\n","%cd /kaggle/working/GalTransl\n","\n","!pip install virtualenv\n","!virtualenv env\n","!/kaggle/working/GalTransl/env/bin/pip install -r requirements.txt\n","!/kaggle/working/GalTransl/env/bin/pip install opencc==1.1.6\n","\n","%cd /kaggle/working/Sakura-13B-Galgame\n","\n","!pip install \"diskcache>=5.6.1\"\n","!pip install llama-cpp-python -i https://sakurallm.github.io/llama-cpp-python/whl/cu121\n","!pip install -q -r requirements.llamacpp.txt\n","!pip install -q pyngrok\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#REPO = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9-GGUF\"\n","#MODEL = \"sakura-14b-qwen2beta-v0.9-iq4_xs_ver2\"\n","#REPO = \"SakuraLLM/Sakura-14B-LNovel-v0.9b-GGUF\"\n","#MODEL = \"sakura-13b-lnovel-v0.9b-Q4_K_M\"\n","\n","#REPO = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9.2-GGUF\"\n","#MODEL = \"sakura-14b-qwen2beta-v0.9.2-iq4xs\"\n","#MODEL = \"sakura-14b-qwen2beta-v0.9.2-q6k\"\n","\n","REPO=\"SakuraLLM/GalTransl-7B-v2\"\n","MODEL=\"Galtransl-7B-v2-Q6_K\"\n","\n","\n","ngrokToken = \"\"\n","\n","# 启动sakura模型\n","%cd /kaggle/working/Sakura-13B-Galgame\n","from huggingface_hub import hf_hub_download\n","from pathlib import Path\n","MODEL_PATH = f\"./models/{MODEL}.gguf\"\n","if not Path(MODEL_PATH).exists():\n","    hf_hub_download(repo_id=REPO, filename=f\"{MODEL}.gguf\", local_dir=\"models/\")\n","\n","import subprocess\n","import threading\n","import time\n","\n","def local_model(dic_status):\n","    cmd = f\"python server.py --model_name_or_path {MODEL_PATH} --llama_cpp --use_gpu --model_version 0.9 --trust_remote_code --no-auth\"\n","    \n","    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n","    for line in p.stderr:\n","        if not dic_status['status']:\n","            s2 = line.decode()\n","            if \"INFO Running on http://127.0.0.1:5000\" in s2:\n","                dic_status['status'] = True\n","            dic_status['msg'] = s2\n","        #print(line.decode(), end='')\n","dic_status = {'status': False, 'msg': ''}\n","\n","threading.Thread(target=local_model, daemon=True, args=(dic_status,)).start()\n","\n","s = \"\"\n","while not dic_status['status']:\n","    if s != dic_status['msg']:\n","        s = dic_status['msg']\n","        print(s)\n","print(\"模型启动成功\")\n","\n","\n","# ngrok：本地HTTP服务用于下载\n","PORT = 8000\n","DIRECTORY = \"/kaggle/working\"\n","def local_http():\n","    import http.server\n","    import socketserver\n","\n","    Handler = http.server.SimpleHTTPRequestHandler\n","    Handler.directory = DIRECTORY\n","\n","    with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n","        print(\"HTTP server is running at port\", PORT)\n","        print(\"Server directory is\", DIRECTORY)\n","        httpd.serve_forever()\n","\n","if ngrokToken:\n","    threading.Thread(target=local_http, daemon=True, args=()).start()\n","        \n","    from pyngrok import conf, ngrok\n","    conf.get_default().auth_token = ngrokToken\n","    conf.get_default().monitor_thread = False\n","    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n","    if len(ssh_tunnels) == 0:\n","        ssh_tunnel = ngrok.connect(PORT)\n","        print('address：'+ssh_tunnel.public_url)\n","    else:\n","        print('address：'+ssh_tunnels[0].public_url)\n","\n","\n","try:\n","    result = subprocess.run(\n","        [\"/kaggle/working/GalTransl/env/bin/python\", \n","        \"-m\", \"GalTransl\",\n","        \"-p\", \"/kaggle/working/galtransl_project\",\n","        \"-t\" \"galtransl-v2\"],\n","        cwd=\"/kaggle/working/GalTransl\",\n","        stdout=subprocess.DEVNULL,  # 屏蔽标准输出\n","        stderr=subprocess.DEVNULL,  # 屏蔽错误输出\n","        timeout=int(11.5*3600)\n","    )\n","    print(\"Process completed successfully.\")\n","except subprocess.TimeoutExpired:\n","    print(\"Process timed out and was terminated.\")\n","    \n","# %cd /kaggle/working/GalTransl\n","# !/kaggle/working/GalTransl/env/bin/python -m GalTransl -p /kaggle/working/galtransl_project -t galtransl-v2\n","\n","import shutil\n","shutil.make_archive('/kaggle/working/result', 'zip', '/kaggle/working/galtransl_project')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5263619,"sourceId":8760771,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
