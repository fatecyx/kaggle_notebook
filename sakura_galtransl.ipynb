{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8760771,"sourceType":"datasetVersion","datasetId":5263619}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/galtransl-project2 /kaggle/working/galtransl\n\n!git clone https://github.com/xd2333/GalTransl.git\n!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n    \n%cd /kaggle/working/GalTransl\n!pip install -r requirements.txt\n\n%cd /kaggle/working/Sakura-13B-Galgame\n\n!pip install \"diskcache>=5.6.1\"\n!pip install llama-cpp-python -i https://abetlen.github.io/llama-cpp-python/whl/cu121\n!pip install -q -r requirements.llamacpp.txt\n!pip install -q pyngrok\n!pip install opencc==1.1.6","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#REPO = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9-GGUF\"\n#MODEL = \"sakura-14b-qwen2beta-v0.9-iq4_xs_ver2\"\n#REPO = \"SakuraLLM/Sakura-14B-LNovel-v0.9b-GGUF\"\n#MODEL = \"sakura-13b-lnovel-v0.9b-Q4_K_M\"\n\n#REPO = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9.2-GGUF\"\n#MODEL = \"sakura-14b-qwen2beta-v0.9.2-iq4xs\"\n#MODEL = \"sakura-14b-qwen2beta-v0.9.2-q6k\"\n\nREPO = \"SakuraLLM/GalTransl-v1\"\nMODEL=\"GalTransl-7B-v1-Q6_K\"\n\nngrokToken = \"\"\n\n# 启动sakura模型\n%cd /kaggle/working/Sakura-13B-Galgame\nfrom huggingface_hub import hf_hub_download\nfrom pathlib import Path\nMODEL_PATH = f\"./models/{MODEL}.gguf\"\nif not Path(MODEL_PATH).exists():\n    hf_hub_download(repo_id=REPO, filename=f\"{MODEL}.gguf\", local_dir=\"models/\")\n\nimport subprocess\nimport threading\nimport time\n\ndef local_model(dic_status):\n    cmd = f\"python server.py --model_name_or_path {MODEL_PATH} --llama_cpp --use_gpu --model_version 0.9 --trust_remote_code --no-auth\"\n    \n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n    for line in p.stderr:\n        if not dic_status['status']:\n            s2 = line.decode()\n            if \"INFO Running on http://127.0.0.1:5000\" in s2:\n                dic_status['status'] = True\n            dic_status['msg'] = s2\n        #print(line.decode(), end='')\ndic_status = {'status': False, 'msg': ''}\n\nthreading.Thread(target=local_model, daemon=True, args=(dic_status,)).start()\n\n# ngrok：本地HTTP服务用于下载\nPORT = 8000\nDIRECTORY = \"/kaggle/working\"\ndef local_http():\n    import http.server\n    import socketserver\n\n    Handler = http.server.SimpleHTTPRequestHandler\n    Handler.directory = DIRECTORY\n\n    with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n        print(\"HTTP server is running at port\", PORT)\n        print(\"Server directory is\", DIRECTORY)\n        httpd.serve_forever()\n\nif ngrokToken:\n    threading.Thread(target=local_http, daemon=True, args=()).start()\n        \n    from pyngrok import conf, ngrok\n    conf.get_default().auth_token = ngrokToken\n    conf.get_default().monitor_thread = False\n    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n    if len(ssh_tunnels) == 0:\n        ssh_tunnel = ngrok.connect(PORT)\n        print('address：'+ssh_tunnel.public_url)\n    else:\n        print('address：'+ssh_tunnels[0].public_url)\n\n\ns = \"\"\nwhile not dic_status['status']:\n    if s != dic_status['msg']:\n        s = dic_status['msg']\n        print(s)\nprint(\"模型启动成功\")\n\n\n%cd /kaggle/working/GalTransl\n!python -m GalTransl -p /kaggle/working/galtransl -t galtransl-v1 --debug-level error\n\nimport shutil\nshutil.make_archive('/kaggle/working/result', 'zip', '/kaggle/working/galtransl')","metadata":{},"execution_count":null,"outputs":[]}]}