{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/Isotr0py/SakuraLLM-Notebooks/blob/main/Sakura-13B-Galgame-Kaggle-llama.cpp.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-27T12:39:24.065177Z","iopub.status.busy":"2023-12-27T12:39:24.064891Z","iopub.status.idle":"2023-12-27T12:44:04.909267Z","shell.execute_reply":"2023-12-27T12:44:04.907937Z","shell.execute_reply.started":"2023-12-27T12:39:24.065151Z"},"trusted":true},"outputs":[],"source":["%cd /kaggle/working\n","!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n","\n","!if [ -d \"/kaggle/working/sakura_translators\" ]; then rm -rf \"/kaggle/working/sakura_translators\"; fi\n","!git clone https://github.com/fatecyx/sakura_translators.git\n","\n","%cd Sakura-13B-Galgame\n","\n","!pip install \"diskcache>=5.6.1\"\n","!pip install llama-cpp-python -i https://abetlen.github.io/llama-cpp-python/whl/cu121\n","!pip install -q -r requirements.llamacpp.txt\n","!pip install -q pyngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-27T12:44:04.911814Z","iopub.status.busy":"2023-12-27T12:44:04.911507Z"},"trusted":true},"outputs":[],"source":["#REPO = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9-GGUF\"\n","#MODEL = \"sakura-14b-qwen2beta-v0.9-iq4_xs_ver2\"\n","#REPO = \"SakuraLLM/Sakura-14B-LNovel-v0.9b-GGUF\"\n","#MODEL = \"sakura-13b-lnovel-v0.9b-Q4_K_M\"\n","REPO = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9.2-GGUF\"\n","#MODEL = \"sakura-14b-qwen2beta-v0.9.2-iq4xs\"\n","MODEL = \"sakura-14b-qwen2beta-v0.9.2-q6k\"\n","\n","ngrokToken = \"\"\n","\n","\n","%cd /kaggle/working/Sakura-13B-Galgame\n","from huggingface_hub import hf_hub_download\n","from pathlib import Path\n","MODEL_PATH = f\"./models/{MODEL}.gguf\"\n","if not Path(MODEL_PATH).exists():\n","    hf_hub_download(repo_id=REPO, filename=f\"{MODEL}.gguf\", local_dir=\"models/\")\n","\n","import subprocess\n","import threading\n","import time\n","\n","def local_model(dic_status):\n","    cmd = f\"python server.py --model_name_or_path {MODEL_PATH} --llama_cpp --use_gpu --model_version 0.9 --trust_remote_code --no-auth\"\n","    \n","    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n","    for line in p.stderr:\n","        if not dic_status['status']:\n","            s2 = line.decode()\n","            if \"INFO Running on http://127.0.0.1:5000\" in s2:\n","                dic_status['status'] = True\n","            dic_status['msg'] = s2\n","        #print(line.decode(), end='')\n","dic_status = {'status': False, 'msg': ''}\n","\n","threading.Thread(target=local_model, daemon=True, args=(dic_status,)).start()\n","\n","PORT = 8000\n","DIRECTORY = \"/kaggle/working\"\n","def local_http():\n","    import http.server\n","    import socketserver\n","\n","    Handler = http.server.SimpleHTTPRequestHandler\n","    Handler.directory = DIRECTORY\n","\n","    with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n","        print(\"HTTP server is running at port\", PORT)\n","        print(\"Server directory is\", DIRECTORY)\n","        httpd.serve_forever()\n","\n","if ngrokToken:\n","    threading.Thread(target=local_http, daemon=True, args=()).start()\n","        \n","    from pyngrok import conf, ngrok\n","    conf.get_default().auth_token = ngrokToken\n","    conf.get_default().monitor_thread = False\n","    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n","    if len(ssh_tunnels) == 0:\n","        ssh_tunnel = ngrok.connect(PORT)\n","        print('address：'+ssh_tunnel.public_url)\n","    else:\n","        print('address：'+ssh_tunnels[0].public_url)\n","\n","\n","s = \"\"\n","while not dic_status['status']:\n","    if s != dic_status['msg']:\n","        s = dic_status['msg']\n","        print(s)\n","print(\"模型启动成功\")\n","\n","\n","%cd /kaggle/working\n","!python /kaggle/working/sakura_translators/sakura_translator4.py \\\n","    --api_host http://127.0.0.1:5000 \\\n","    --src_folder /kaggle/input/srcdata1 \\\n","    --dst_folder /kaggle/working/output \\\n","    --log /kaggle/working/log.txt \\\n","    --version 0.9 --replace_file /kaggle/working/dict.txt \\\n","    --engine rpgmv --context_lines 0 --batch_lines 1 \\\n","    --zip /kaggle/working/zip_out \\\n","    --delete --silent\n","\n","# !python /kaggle/working/sakura_translators/sakura_translator2.py \n","#     --api_host http://127.0.0.1:5000 \\\n","#     --src_folder /kaggle/input/srcdata1 \\\n","#     --dst_folder /kaggle/working/output \\\n","#     --log /kaggle/working/log.txt \\\n","#     --replace_file /kaggle/working/dict.json \\\n","#     --zip /kaggle/working/zip_out \\\n","#     --delete --silent"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4208491,"sourceId":7261583,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
